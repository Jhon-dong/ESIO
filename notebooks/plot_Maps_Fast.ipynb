{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Plot forecast maps with all available models.\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import json\n",
    "from esio import EsioData as ed\n",
    "from esio import ice_plot\n",
    "from esio import import_data\n",
    "import subprocess\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import timeit\n",
    "\n",
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=.8, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_small_contours(p, thres=10):\n",
    "    for level in p.collections:\n",
    "        for kp,path in reversed(list(enumerate(level.get_paths()))):\n",
    "            # go in reversed order due to deletions!\n",
    "\n",
    "            # include test for \"smallness\" of your choice here:\n",
    "            # I'm using a simple estimation for the diameter based on the\n",
    "            #    x and y diameter...\n",
    "            verts = path.vertices # (N,2)-shape array of contour line coordinates\n",
    "            diameter = np.max(verts.max(axis=0) - verts.min(axis=0))\n",
    "\n",
    "            if diameter<thres: # threshold to be refined for your actual dimensions!\n",
    "                del(level.get_paths()[kp])  # no remove() for Path objects:(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_figure_init_times(fig_dir):\n",
    "    # Get list of all figures\n",
    "    fig_files = glob.glob(os.path.join(fig_dir,'*.png'))\n",
    "    init_times = list(reversed(sorted(list(set([os.path.basename(x).split('_')[3] for x in fig_files])))))\n",
    "    return init_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_figure_init_times( os.path.join(ed.EsioData.load().fig_dir, 'model', 'all_model', 'sic', 'maps_weekly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(ds_status=None, fig_dir=None, int_2_days_dict=None, NweeksUpdate=3):\n",
    "    # Get list of all figures\n",
    "    fig_files = glob.glob(os.path.join(fig_dir,'*.png'))\n",
    "    # For each figure\n",
    "    for fig_f in fig_files:\n",
    "        # Get the init_time from file name\n",
    "        cit = os.path.basename(fig_f).split('_')[3]\n",
    "        # Get the forecast int from file name\n",
    "        cft = int(os.path.basename(fig_f).split('_')[4].split('.')[0])\n",
    "        # Check if current it and ft were requested, otherwise skip\n",
    "        if (np.datetime64(cit) in ds_status.init_time.values) & (np.timedelta64(int_2_days_dict[cft]) in ds_status.fore_time.values):\n",
    "            # Always update the last 3 weeks (some models have lagg before we get them)\n",
    "            # Check if cit is one of the last NweeksUpdate init times in init_time\n",
    "            if (np.datetime64(cit) not in ds_status.init_time.values[-NweeksUpdate:]):\n",
    "                ds_status.status.loc[dict(init_time=cit, fore_time=int_2_days_dict[cft])] = 1\n",
    "        \n",
    "    return ds_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_PanArctic_Maps():\n",
    "    # Plotting Info\n",
    "    runType = 'forecast'\n",
    "    variables = ['sic']\n",
    "    metrics_all = {'sic':['anomaly','mean','SIP'], 'hi':['mean']}\n",
    "    #metrics_all = {'sic':['SIP']}\n",
    "    updateAll = False\n",
    "    # Some models are terrible/have serious issues, so don't include in MME\n",
    "    MME_NO = ['hcmr']\n",
    "\n",
    "    # Define Init Periods here, spaced by 7 days (aprox a week)\n",
    "    # Now\n",
    "    cd = datetime.datetime.now()\n",
    "    cd = datetime.datetime(cd.year, cd.month, cd.day) # Set hour min sec to 0. \n",
    "    # Hardcoded start date (makes incremental weeks always the same)\n",
    "    start_t = datetime.datetime(1950, 1, 1) # datetime.datetime(1950, 1, 1)\n",
    "    # Parms for this plot\n",
    "    Ndays = 7 # time period to aggregate maps to\n",
    "    Npers = 12 # number of periods to plot (from current date)\n",
    "    init_slice = np.arange(start_t, cd, datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "    init_slice = init_slice[-Npers:] # Select only the last Npers of periods (weeks) since current date\n",
    "\n",
    "    # Forecast times to plot\n",
    "    weeks = pd.to_timedelta(np.arange(0,5,1), unit='W')\n",
    "    months = pd.to_timedelta(np.arange(2,12,1), unit='M')\n",
    "    years = pd.to_timedelta(np.arange(1,2), unit='Y') - np.timedelta64(1, 'D') # need 364 not 365\n",
    "    slices = weeks.union(months).union(years).round('1d')\n",
    "    da_slices = xr.DataArray(slices, dims=('fore_time'))\n",
    "    da_slices.fore_time.values.astype('timedelta64[D]')\n",
    "\n",
    "    # Help conversion between \"week/month\" period used for figure naming and the actual forecast time delta value\n",
    "    int_2_days_dict = dict(zip(np.arange(0,da_slices.size), da_slices.values))\n",
    "    days_2_int_dict = {v: k for k, v in int_2_days_dict.items()}\n",
    "\n",
    "    #############################################################\n",
    "    # Load in Data\n",
    "    #############################################################\n",
    "\n",
    "    E = ed.EsioData.load()\n",
    "    # Get median ice edge by DOY\n",
    "    median_ice_fill = xr.open_mfdataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'ice_edge.nc')).sic\n",
    "    # Get mean sic by DOY\n",
    "    mean_1980_2010_sic = xr.open_dataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'mean_1980_2010_sic.nc')).sic\n",
    "    ds_81 = xr.open_mfdataset(E.obs['NSIDC_0081']['sipn_nc']+'_yearly/*.nc', concat_dim='time', autoclose=True, parallel=True)#,\n",
    "\n",
    "\n",
    "    # Define models to plot\n",
    "    models_2_plot = list(E.model.keys())\n",
    "    models_2_plot = [x for x in models_2_plot if x!='piomas'] # remove some models\n",
    "    models_2_plot = [x for x in models_2_plot if E.icePredicted[x]] # Only predictive models\n",
    "\n",
    "    # Get # of models and setup subplot dims\n",
    "    Nmod = len(models_2_plot) + 2 #(+2 for obs and MME)\n",
    "    Nr = int(np.floor(np.sqrt(Nmod)))\n",
    "    Nc = int(np.ceil(Nmod/Nr))\n",
    "    assert Nc*Nr>=Nmod, 'Need more subplots'\n",
    "\n",
    "    for cvar in variables:\n",
    "\n",
    "        # Define fig dir and make if doesn't exist\n",
    "        fig_dir = os.path.join(E.fig_dir, 'model', 'all_model', cvar, 'maps_weekly')\n",
    "        if not os.path.exists(fig_dir):\n",
    "            os.makedirs(fig_dir)\n",
    "\n",
    "        # Make requested dataArray as specified above\n",
    "        ds_status = xr.DataArray(np.ones((init_slice.size, da_slices.size))*np.NaN, dims=('init_time','fore_time'), coords={'init_time':init_slice,'fore_time':da_slices}) \n",
    "        ds_status.name = 'status'\n",
    "        ds_status = ds_status.to_dataset()\n",
    "\n",
    "        # Check what plots we already have\n",
    "        if ~updateAll:\n",
    "            ds_status = update_status(ds_status=ds_status, fig_dir=fig_dir, int_2_days_dict=int_2_days_dict)\n",
    "            print(ds_status.status.values)\n",
    "\n",
    "        # Drop IC/FT we have already plotted (orthoginal only)\n",
    "        ds_status = ds_status.where(ds_status.status.sum(dim='fore_time')<ds_status.fore_time.size, drop=True)\n",
    "\n",
    "        print(\"Starting plots...\")\n",
    "        # For each init_time we haven't plotted yet\n",
    "        start_time_cmod = timeit.default_timer()\n",
    "        for it in ds_status.init_time.values: \n",
    "            print(it)\n",
    "            it_start = it-np.timedelta64(Ndays,'D') # Start period for init period (it is end of period)\n",
    "\n",
    "            # For each forecast time we haven't plotted yet\n",
    "            ft_to_plot = ds_status.sel(init_time=it)\n",
    "            ft_to_plot = ft_to_plot.where(ft_to_plot.isnull(), drop=True).fore_time\n",
    "            for ft in ft_to_plot.values: \n",
    "                print(ft.astype('timedelta64[D]'))\n",
    "                cs_str = format(days_2_int_dict[ft], '02')\n",
    "                cdoy = pd.to_datetime(it + ft).timetuple().tm_yday\n",
    "                it_yr = str(pd.to_datetime(it).year)\n",
    "                it_m = str(pd.to_datetime(it).month)\n",
    "\n",
    "                # Get datetime64 of valid time start and end\n",
    "                valid_start = it_start + ft\n",
    "                valid_end = it + ft\n",
    "\n",
    "                # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "                for metric in metrics_all[cvar]:\n",
    "\n",
    "                    # Set up plotting info\n",
    "                    if cvar=='sic':\n",
    "                        if metric=='mean':\n",
    "                            cmap_c = matplotlib.colors.ListedColormap(sns.color_palette(\"Blues_r\", 10))\n",
    "                            cmap_c.set_bad(color = 'lightgrey')\n",
    "                            c_label = 'Sea Ice Concentration (-)'\n",
    "                            c_vmin = 0\n",
    "                            c_vmax = 1\n",
    "                        elif metric=='SIP':\n",
    "                            cmap_c = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"white\",\"orange\",\"red\",\"#990000\"])\n",
    "                            cmap_c.set_bad(color = 'lightgrey')\n",
    "                            c_label = 'Sea Ice Probability (-)'\n",
    "                            c_vmin = 0\n",
    "                            c_vmax = 1\n",
    "                        elif metric=='anomaly':\n",
    "    #                         cmap_c = matplotlib.colors.ListedColormap(sns.color_palette(\"coolwarm\", 9))\n",
    "                            cmap_c = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\",\"white\",\"blue\"])\n",
    "                            cmap_c.set_bad(color = 'lightgrey')\n",
    "                            c_label = 'SIC Anomaly to 1980-2010 Mean'\n",
    "                            c_vmin = -1\n",
    "                            c_vmax = 1\n",
    "\n",
    "                    elif cvar=='hi':\n",
    "                        if metric=='mean':\n",
    "                            cmap_c = matplotlib.colors.ListedColormap(sns.color_palette(\"Reds_r\", 10))\n",
    "                            cmap_c.set_bad(color = 'lightgrey')\n",
    "                            c_label = 'Sea Ice Thickness (m)'\n",
    "                            c_vmin = 0\n",
    "                            c_vmax = None\n",
    "                    else:\n",
    "                        raise ValueError(\"cvar not found.\") \n",
    "\n",
    "\n",
    "                    MME_list = []            \n",
    "                    # New Plot\n",
    "                    start_time_plot = timeit.default_timer()\n",
    "                    (f, axes) = ice_plot.multi_polar_axis(ncols=Nc, nrows=Nr, Nplots=Nmod)\n",
    "\n",
    "                    # Plot Obs (if available)\n",
    "                    ax_num = 0\n",
    "                    if ((it + ft) in ds_81.time.values):\n",
    "\n",
    "                        if metric=='mean':\n",
    "                            da_obs_c = ds_81.sic.sel(time=(it + ft))\n",
    "                        elif metric=='SIP':\n",
    "                            da_obs_c = (ds_81.sic.sel(time=(it + ft)) >=0.15).astype('int').where(ds_81.sic.sel(time=(it + ft)).notnull())\n",
    "                        elif metric=='anomaly':\n",
    "                            da_obs_VT = ds_81.sic.sel(time=(it + ft))\n",
    "                            da_obs_mean = mean_1980_2010_sic.isel(time=cdoy)\n",
    "                            da_obs_c = da_obs_VT - da_obs_mean\n",
    "                        else:\n",
    "                            raise ValueError('Not implemented')\n",
    "                        da_obs_c.plot.pcolormesh(ax=axes[ax_num], x='lon', y='lat', \n",
    "                                              transform=ccrs.PlateCarree(),\n",
    "                                              add_colorbar=False,\n",
    "                                              cmap=cmap_c,\n",
    "                                              vmin=c_vmin, vmax=c_vmax)\n",
    "\n",
    "                        # Overlay median ice edge\n",
    "                        #if metric=='mean':\n",
    "                            #po = median_ice_fill.isel(time=cdoy).plot.contour(ax=axes[ax_num], x='xm', y='ym', \n",
    "                                                                              #=('#bc0f60'),\n",
    "                                                                              #linewidths=[0.5],\n",
    "                                                                              #levels=[0.5])\n",
    "                            #remove_small_contours(po, thres=10)\n",
    "                    else: # When were in the future (or obs are missing)\n",
    "                        if metric=='anomaly': # Still get climatological mean for model difference\n",
    "                            da_obs_mean = mean_1980_2010_sic.isel(time=cdoy)\n",
    "                    axes[ax_num].set_title('Observed')\n",
    "\n",
    "\n",
    "                    # Plot all Models\n",
    "                    p = None # initlaize to know if we found any data\n",
    "                    for (i, cmod) in enumerate(models_2_plot):\n",
    "                        print(cmod)\n",
    "                        i = i+2 # shift for obs and MME\n",
    "                        axes[i].set_title(E.model[cmod]['model_label'])\n",
    "\n",
    "                        # Load in Model\n",
    "                        # Find only files that have current year and month in filename (speeds up loading)\n",
    "                        all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*'+it_yr+'*'+it_m+'*.nc') \n",
    "\n",
    "                        # Check we have files \n",
    "                        files = glob.glob(all_files)\n",
    "                        if not files:\n",
    "                            #print(\"Skipping model\", cmod, \"no forecast files found.\")\n",
    "                            continue # Skip this model\n",
    "\n",
    "                        # Load in model    \n",
    "                        #start_time = timeit.default_timer()\n",
    "                        ds_model = xr.open_mfdataset(sorted(files), \n",
    "                                                     chunks={'fore_time': 1, 'init_time': 1, 'nj': 304, 'ni': 448},  \n",
    "                                                     concat_dim='init_time', autoclose=True, parallel=True)\n",
    "                        ds_model.rename({'nj':'x', 'ni':'y'}, inplace=True)\n",
    "                        #print(\"Loading took  \", (timeit.default_timer() - start_time), \" seconds.\")\n",
    "\n",
    "                        # Select init period and fore_time of interest\n",
    "                        #start_time = timeit.default_timer()\n",
    "                        ds_model = ds_model.sel(init_time=slice(it_start, it))\n",
    "                        # Check we found any init_times in range\n",
    "                        if ds_model.init_time.size==0:\n",
    "                            print('init_time not found.')\n",
    "                            continue\n",
    "                        #print(\"Selecting init_time took  \", (timeit.default_timer() - start_time), \" seconds.\")\n",
    "\n",
    "                        # Select var of interest (if available)\n",
    "                        if cvar in ds_model.variables:\n",
    "        #                     print('found ',cvar)\n",
    "                            ds_model = ds_model[cvar]\n",
    "                        else:\n",
    "                            print('cvar not found.')\n",
    "                            continue\n",
    "\n",
    "                        #start_time = timeit.default_timer()\n",
    "                        # Get Valid time\n",
    "                        ds_model = import_data.get_valid_time(ds_model)\n",
    "\n",
    "                        # Check if we have any valid times in range of target dates\n",
    "                        ds_model = ds_model.where((ds_model.valid_time>=valid_start) & (ds_model.valid_time<=valid_end), drop=True) \n",
    "                        if ds_model.fore_time.size == 0:\n",
    "                            print(\"no fore_time found for target period.\")\n",
    "                            continue\n",
    "\n",
    "                        # Average over for_time and init_times\n",
    "                        ds_model = ds_model.mean(dim=['fore_time','init_time'])\n",
    "\n",
    "    #                     if ft in ds_model.fore_time.values:\n",
    "    #                         ds_model = ds_model.sel(fore_time=ft)\n",
    "    #                     else:\n",
    "    #                         print('fore_time not found.')\n",
    "    #                         continue\n",
    "                        #print(\"Selecting fore_time took \", (timeit.default_timer() - start_time), \" seconds.\")\n",
    "\n",
    "                        #start_time = timeit.default_timer()\n",
    "        #                 print(\"Found data for model \", cmod, \". Plotting...\")    \n",
    "                        if metric=='mean': # Calc ensemble mean\n",
    "                            ds_model = ds_model.mean(dim='ensemble')\n",
    "                        elif metric=='SIP': # Calc probability\n",
    "                            # Issue of some ensemble members having missing data\n",
    "    #                         ds_model = ds_model.where(ds_model>=0.15, other=0).mean(dim='ensemble')\n",
    "                            ok_ens = ((ds_model.notnull().sum(dim='x').sum(dim='y'))>0) # select ensemble members with any data\n",
    "                            ds_model = ((ds_model.where(ok_ens, drop=True)>=0.15) ).mean(dim='ensemble').where(ds_model.isel(ensemble=0).notnull())\n",
    "                        elif metric=='anomaly': # Calc anomaly in reference to mean observed 1980-2010\n",
    "                            ds_model = ds_model.mean(dim='ensemble') - da_obs_mean\n",
    "                            # Add back lat/long (get dropped because of round off differences)\n",
    "                            ds_model['lat'] = da_obs_mean.lat\n",
    "                            ds_model['lon'] = da_obs_mean.lon\n",
    "                        else:\n",
    "                            raise ValueError('metric not implemented')\n",
    "                        #print(\"Calc metric took  \", (timeit.default_timer() - start_time), \" seconds.\")\n",
    "\n",
    "                        # Build MME\n",
    "                        if 'ensemble' in ds_model:\n",
    "                            ds_model = ds_model.drop('ensemble')\n",
    "                        if cmod not in MME_NO: # Exclude some models (bad) from MME\n",
    "                            MME_list.append(ds_model)\n",
    "\n",
    "                        # Plot\n",
    "                        #start_time = timeit.default_timer()\n",
    "                        p = ds_model.plot.pcolormesh(ax=axes[i], x='lon', y='lat', \n",
    "                                              transform=ccrs.PlateCarree(),\n",
    "                                              add_colorbar=False,\n",
    "                                              cmap=cmap_c,\n",
    "                                              vmin=c_vmin, vmax=c_vmax)\n",
    "                        #print(\"Plotting took  \", (timeit.default_timer() - start_time), \" seconds.\")\n",
    "\n",
    "                        # Overlay median ice edge\n",
    "                        #if metric=='mean':\n",
    "                            #po = median_ice_fill.isel(time=cdoy).plot.contour(ax=axes[i], x='xm', y='ym', \n",
    "                                                                              #colors=('#bc0f60'),\n",
    "                                                                              #linewidths=[0.5],\n",
    "                                                                              #levels=[0.5]) #, label='Median ice edge 1981-2010')\n",
    "                            #remove_small_contours(po, thres=10)\n",
    "\n",
    "                        axes[i].set_title(E.model[cmod]['model_label'])\n",
    "\n",
    "                        # Clean up for current model\n",
    "                        ds_model = None\n",
    "\n",
    "\n",
    "\n",
    "                    # MME\n",
    "                    ax_num = 1\n",
    "                    if MME_list: # If we had any models for this time\n",
    "                        # Concat over all models\n",
    "                        ds_MME = xr.concat(MME_list, dim='model')\n",
    "                        # Take average\n",
    "                        ds_MME = ds_MME.mean(dim='model')\n",
    "                        # Mask out using OBSERVED LAND MASK\n",
    "                        # TODO: should happen if all models have land mask.... fix upstream\n",
    "                        pmme = ds_MME.plot.pcolormesh(ax=axes[ax_num], x='lon', y='lat', \n",
    "                                                      transform=ccrs.PlateCarree(),\n",
    "                                                      add_colorbar=False, \n",
    "                                                      cmap=cmap_c,vmin=c_vmin, vmax=c_vmax)\n",
    "                        # Overlay median ice edge\n",
    "                        #if metric=='mean':\n",
    "                            #po = median_ice_fill.isel(time=cdoy).plot.contour(ax=axes[ax_num], x='xm', y='ym', \n",
    "    #                                                                           colors=('#bc0f60'),\n",
    "    #                                                                           linewidths=[0.5],\n",
    "    #                                                                           levels=[0.5])\n",
    "                            #remove_small_contours(po, thres=10)\n",
    "                    axes[ax_num].set_title('MME')\n",
    "\n",
    "                    # Make pretty\n",
    "                    f.subplots_adjust(right=0.8)\n",
    "                    cbar_ax = f.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "                    if p:\n",
    "                        cbar = f.colorbar(p, cax=cbar_ax, label=c_label)\n",
    "                        if metric=='anomaly':\n",
    "                            cbar.set_ticks(np.arange(-1,1.1,0.2))\n",
    "                        else:\n",
    "                            cbar.set_ticks(np.arange(0,1.1,0.1))\n",
    "                        #cbar.set_ticklabels(np.arange(0,1,0.05))\n",
    "\n",
    "                    # Set title of all plots\n",
    "                    init_time_2 =  pd.to_datetime(it).strftime('%Y-%m-%d')\n",
    "                    init_time_1 =  pd.to_datetime(it_start).strftime('%Y-%m-%d')\n",
    "                    valid_time_2 = pd.to_datetime(it+ft).strftime('%Y-%m-%d')\n",
    "                    valid_time_1 = pd.to_datetime(it_start+ft).strftime('%Y-%m-%d')\n",
    "                    plt.suptitle('Initialization Time: '+init_time_1+' to '+init_time_2+'\\n Valid Time: '+valid_time_1+' to '+valid_time_2+'\\n Week '+cs_str,\n",
    "                                 fontsize=15)\n",
    "                    plt.subplots_adjust(top=0.85)\n",
    "\n",
    "                    # Save to file\n",
    "                    f_out = os.path.join(fig_dir,'panArctic_'+metric+'_'+runType+'_'+init_time_2+'_'+cs_str+'.png')\n",
    "                    f.savefig(f_out,bbox_inches='tight', dpi=200)\n",
    "                    print(\"saved \", f_out)\n",
    "                    print(\"Figure took  \", (timeit.default_timer() - start_time_plot)/60, \" minutes.\")\n",
    "\n",
    "                    # Mem clean up\n",
    "                    plt.close(f)\n",
    "                    p = None\n",
    "                    ds_MME= None\n",
    "                    da_obs_c = None\n",
    "                    da_obs_mean = None\n",
    "\n",
    "            # Done with current it\n",
    "            print(\"Took \", (timeit.default_timer() - start_time_cmod)/60, \" minutes.\")\n",
    "\n",
    "    # Down with plots now:\n",
    "    # 1) Update json file\n",
    "    # 2) Update gifs\n",
    "\n",
    "#     # Make requested dataArray as specified above\n",
    "#     ds_status = xr.DataArray(np.ones((init_slice.size, da_slices.size))*np.NaN, dims=('init_time','fore_time'), coords={'init_time':init_slice,'fore_time':da_slices}) \n",
    "#     ds_status.name = 'status'\n",
    "#     ds_status = ds_status.to_dataset()\n",
    "\n",
    "#     # Check what plots we already have\n",
    "#     if ~updateAll:\n",
    "#         ds_status = update_status(ds_status=ds_status, fig_dir=fig_dir, int_2_days_dict=int_2_days_dict)\n",
    "#         print(ds_status.status.values)\n",
    "\n",
    "\n",
    "    json_format = get_figure_init_times(fig_dir)\n",
    "\n",
    "    # Update json file that keeps track of what plots we have\n",
    "#     init_plotted = ds_status.status.sum(dim='fore_time')\n",
    "#     init_plotted = list(reversed(sorted(init_plotted.where(init_plotted>0, drop=True).init_time.values)))\n",
    "\n",
    "#     json_format = [pd.to_datetime(x).strftime('%Y-%m-%d') for x in init_plotted]\n",
    "    json_dict = [{\"date\":cd,\"label\":cd} for cd in json_format]\n",
    "\n",
    "    json_f = os.path.join(fig_dir, 'plotdates_current.json')\n",
    "    with open(json_f, 'w') as outfile:\n",
    "        json.dump(json_dict, outfile)\n",
    "\n",
    "    # Make into Gifs\n",
    "    # TODO fig_dir hardcoded to current variable\n",
    "    for cit in json_format:\n",
    "        subprocess.call(str(\"/home/disk/sipn/nicway/python/ESIO/scripts/makeGif.sh \" + fig_dir + \" \" + cit), shell=True)\n",
    "\n",
    "    print(\"Finished plotting panArctic Maps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Start up Client\n",
    "    client = Client()\n",
    "    \n",
    "    # Call function\n",
    "    Update_PanArctic_Maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.4 esio",
   "language": "python",
   "name": "esio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
