{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Plot forecast maps with all available models.\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import struct\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import datetime\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import seaborn as sns\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning) # not good to supress but they divide by nan are annoying\n",
    "#warnings.simplefilter(action='ignore', category=UserWarning) # https://github.com/pydata/xarray/issues/2273\n",
    "import json\n",
    "from esio import EsioData as ed\n",
    "from esio import ice_plot\n",
    "from esio import import_data\n",
    "import subprocess\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import timeit\n",
    "\n",
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=.8, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f77e42127b8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#client = Client()\n",
    "#client\n",
    "dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-28T00:00:00.000000000 2018-09-30T00:00:00.000000000\n",
      "\n",
      "<xarray.DataArray (fore_time: 16)>\n",
      "array([                0,   604800000000000,  1209600000000000,\n",
      "        1814400000000000,  2419200000000000,  5270400000000000,\n",
      "        7862400000000000, 10540800000000000, 13132800000000000,\n",
      "       15811200000000000, 18403200000000000, 20995200000000000,\n",
      "       23673600000000000, 26265600000000000, 28944000000000000,\n",
      "       31449600000000000], dtype='timedelta64[ns]')\n",
      "Coordinates:\n",
      "  * fore_time  (fore_time) timedelta64[ns] 0 days 7 days 14 days 21 days ...\n"
     ]
    }
   ],
   "source": [
    "#def Update_PanArctic_Maps():\n",
    "# Plotting Info\n",
    "runType = 'forecast'\n",
    "variables = ['sic']\n",
    "metrics_all = {'sic':['anomaly','mean','SIP'], 'hi':['mean']}\n",
    "#metrics_all = {'sic':['SIP']}\n",
    "updateAll = False\n",
    "\n",
    "# Define Init Periods here, spaced by 7 days (aprox a week)\n",
    "# Now\n",
    "cd = datetime.datetime.now()\n",
    "cd = datetime.datetime(cd.year, cd.month, cd.day) # Set hour min sec to 0. \n",
    "# Hardcoded start date (makes incremental weeks always the same)\n",
    "start_t = datetime.datetime(1950, 1, 1) # datetime.datetime(1950, 1, 1)\n",
    "# Params for this plot\n",
    "Ndays = 7 # time period to aggregate maps to (default is 7)\n",
    "Npers = 36 # number of periods agg (from current date) (default is 14)\n",
    "init_slice = np.arange(start_t, cd, datetime.timedelta(days=Ndays)).astype('datetime64[ns]')\n",
    "init_slice = init_slice[-Npers:] # Select only the last Npers of periods (weeks) since current date\n",
    "print(init_slice[0],init_slice[-1])\n",
    "print('')\n",
    "\n",
    "# Forecast times to plot\n",
    "weeks = pd.to_timedelta(np.arange(0,5,1), unit='W')\n",
    "months = pd.to_timedelta(np.arange(2,12,1), unit='M')\n",
    "years = pd.to_timedelta(np.arange(1,2), unit='Y') - np.timedelta64(1, 'D') # need 364 not 365\n",
    "slices = weeks.union(months).union(years).round('1d')\n",
    "da_slices = xr.DataArray(slices, dims=('fore_time'))\n",
    "da_slices.fore_time.values.astype('timedelta64[D]')\n",
    "print(da_slices)\n",
    "\n",
    "# Help conversion between \"week/month\" period used for figure naming and the actual forecast time delta value\n",
    "int_2_days_dict = dict(zip(np.arange(0,da_slices.size), da_slices.values))\n",
    "days_2_int_dict = {v: k for k, v in int_2_days_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dampedAnomalyTrend',\n",
       " 'gfdlsipn',\n",
       " 'yopp',\n",
       " 'ukmetofficesipn',\n",
       " 'ecmwfsipn',\n",
       " 'ecmwf',\n",
       " 'metreofr',\n",
       " 'ukmo',\n",
       " 'kma',\n",
       " 'ncep',\n",
       " 'usnavysipn',\n",
       " 'usnavyncep',\n",
       " 'rasmesrl',\n",
       " 'noaasipn',\n",
       " 'noaasipn_ext',\n",
       " 'usnavygofs',\n",
       " 'modcansipns_3',\n",
       " 'modcansipns_4',\n",
       " 'szapirosipn',\n",
       " 'awispin',\n",
       " 'nicosipn']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################################\n",
    "# Load in Observed and non-dynamic model Data\n",
    "#############################################################\n",
    "\n",
    "E = ed.EsioData.load()\n",
    "mod_dir = E.model_dir\n",
    "\n",
    "# Get median ice edge by DOY\n",
    "median_ice_fill = xr.open_mfdataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'ice_edge.nc')).sic\n",
    "# Get mean sic by DOY\n",
    "mean_1980_2010_sic = xr.open_dataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'mean_1980_2010_sic.nc')).sic\n",
    "# Get average sip by DOY\n",
    "mean_1980_2010_SIP = xr.open_dataset(os.path.join(E.obs_dir, 'NSIDC_0051', 'agg_nc', 'hist_SIP_1980_2010.nc')).sip    \n",
    "\n",
    "# Get recent observations\n",
    "ds_81 = xr.open_mfdataset(E.obs['NSIDC_0081']['sipn_nc']+'_yearly/*.nc', concat_dim='time', autoclose=True, parallel=True)#,\n",
    "\n",
    "# Define models to plot\n",
    "models_2_plot = list(E.model.keys())\n",
    "models_2_plot = [x for x in models_2_plot if x not in ['piomas','MME','MME_NEW','uclsipn','hcmr']] # remove some models\n",
    "models_2_plot = [x for x in models_2_plot if E.icePredicted[x]] # Only predictive models\n",
    "#models_2_plot = ['usnavyncep']\n",
    "models_2_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_in_time_range(x):\n",
    "    \n",
    "#     if x.sel(init_time=slice(time_bds[0],time_bds[1])).init_time.size>0: # We have some time in the time range\n",
    "#         return x\n",
    "#     else:\n",
    "#         return []\n",
    "# time_bds = [init_slice[0],init_slice[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dampedAnomalyTrend\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "gfdlsipn\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "yopp\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "ukmetofficesipn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 26\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 28\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 31\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 30\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 25\n",
      "  **atop_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "ecmwfsipn\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "ecmwf\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "metreofr\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "ukmo\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "kma\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "ncep\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "usnavysipn\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "usnavyncep\n",
      "2018-01-22T00:00:00.000000000 to 2018-01-28T00:00:00.000000000\n",
      "2018-01-29T00:00:00.000000000 to 2018-02-04T00:00:00.000000000\n",
      "2018-02-05T00:00:00.000000000 to 2018-02-11T00:00:00.000000000\n",
      "2018-02-12T00:00:00.000000000 to 2018-02-18T00:00:00.000000000\n",
      "2018-02-19T00:00:00.000000000 to 2018-02-25T00:00:00.000000000\n",
      "2018-02-26T00:00:00.000000000 to 2018-03-04T00:00:00.000000000\n",
      "2018-03-05T00:00:00.000000000 to 2018-03-11T00:00:00.000000000\n",
      "2018-03-12T00:00:00.000000000 to 2018-03-18T00:00:00.000000000\n",
      "2018-03-19T00:00:00.000000000 to 2018-03-25T00:00:00.000000000\n",
      "2018-03-26T00:00:00.000000000 to 2018-04-01T00:00:00.000000000\n",
      "2018-04-02T00:00:00.000000000 to 2018-04-08T00:00:00.000000000\n",
      "2018-04-09T00:00:00.000000000 to 2018-04-15T00:00:00.000000000\n",
      "2018-04-16T00:00:00.000000000 to 2018-04-22T00:00:00.000000000\n",
      "2018-04-23T00:00:00.000000000 to 2018-04-29T00:00:00.000000000\n",
      "2018-04-30T00:00:00.000000000 to 2018-05-06T00:00:00.000000000\n",
      "2018-05-07T00:00:00.000000000 to 2018-05-13T00:00:00.000000000\n",
      "2018-05-14T00:00:00.000000000 to 2018-05-20T00:00:00.000000000\n",
      "2018-05-21T00:00:00.000000000 to 2018-05-27T00:00:00.000000000\n",
      "2018-05-28T00:00:00.000000000 to 2018-06-03T00:00:00.000000000\n",
      "2018-06-04T00:00:00.000000000 to 2018-06-10T00:00:00.000000000\n",
      "2018-06-11T00:00:00.000000000 to 2018-06-17T00:00:00.000000000\n",
      "2018-06-18T00:00:00.000000000 to 2018-06-24T00:00:00.000000000\n",
      "2018-06-25T00:00:00.000000000 to 2018-07-01T00:00:00.000000000\n",
      "2018-07-02T00:00:00.000000000 to 2018-07-08T00:00:00.000000000\n",
      "2018-07-09T00:00:00.000000000 to 2018-07-15T00:00:00.000000000\n",
      "2018-07-16T00:00:00.000000000 to 2018-07-22T00:00:00.000000000\n",
      "2018-07-23T00:00:00.000000000 to 2018-07-29T00:00:00.000000000\n",
      "2018-07-30T00:00:00.000000000 to 2018-08-05T00:00:00.000000000\n",
      "2018-08-06T00:00:00.000000000 to 2018-08-12T00:00:00.000000000\n",
      "2018-08-13T00:00:00.000000000 to 2018-08-19T00:00:00.000000000\n",
      "2018-08-20T00:00:00.000000000 to 2018-08-26T00:00:00.000000000\n",
      "2018-08-27T00:00:00.000000000 to 2018-09-02T00:00:00.000000000\n",
      "2018-09-03T00:00:00.000000000 to 2018-09-09T00:00:00.000000000\n",
      "2018-09-10T00:00:00.000000000 to 2018-09-16T00:00:00.000000000\n",
      "2018-09-17T00:00:00.000000000 to 2018-09-23T00:00:00.000000000\n",
      "2018-09-24T00:00:00.000000000 to 2018-09-30T00:00:00.000000000\n",
      "rasmesrl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c42c96f643b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                                  \u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0;34m'fore_time'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'init_time'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nj'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m304\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ni'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m448\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                  \u001b[0mconcat_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'init_time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                  parallel=True, drop_variables=drop_vars)\n\u001b[0m\u001b[1;32m     26\u001b[0m                                  \u001b[0;31m# preprocess=lambda x : is_in_time_range(x)) # 'fore_time': 1, ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mds_model_ALL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'nj'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ni'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/esio/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, lock, data_vars, coords, autoclose, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# calling compute here will return the datasets/file_objs lists,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# the underlying datasets will still be stored as dask arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_objs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_objs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;31m# close datasets in case of a ValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/esio/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/esio/lib/python3.6/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     results = get_async(pool.apply_async, len(pool._pool), dsk, result,\n\u001b[1;32m     74\u001b[0m                         \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_thread_get_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                         pack_exception=pack_exception, **kwargs)\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Cleanup pools associated to dead threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/esio/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;31m# Main loop, wait on tasks to finish, insert new ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'waiting'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ready'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'running'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/esio/lib/python3.6/site-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/esio/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/esio/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#          Loop through each dynamical model              #\n",
    "###########################################################\n",
    "\n",
    "# Plot all Models\n",
    "for cmod in models_2_plot:\n",
    "    print(cmod)\n",
    "\n",
    "    # Load in Model\n",
    "    # Find only files that have current year and month in filename (speeds up loading)\n",
    "    all_files = os.path.join(E.model[cmod][runType]['sipn_nc'], '*.nc') \n",
    "\n",
    "    # Check we have files \n",
    "    files = glob.glob(all_files)\n",
    "    if not files:\n",
    "        continue # Skip this model\n",
    "\n",
    "    # Get list of variablse we want to drop\n",
    "    drop_vars = [x for x in xr.open_dataset(sorted(files)[-1],autoclose=True).data_vars if x not in variables]\n",
    "    \n",
    "    # Load in model   \n",
    "    ds_model_ALL = xr.open_mfdataset(sorted(files), \n",
    "                                 chunks={ 'fore_time': 1,'init_time': 1,'nj': 304, 'ni': 448},  \n",
    "                                 concat_dim='init_time', autoclose=True, \n",
    "                                 parallel=True, drop_variables=drop_vars)\n",
    "                                 # preprocess=lambda x : is_in_time_range(x)) # 'fore_time': 1, ,\n",
    "    ds_model_ALL.rename({'nj':'x', 'ni':'y'}, inplace=True)\n",
    "    \n",
    "    #print(ds_model_ALL)\n",
    "    \n",
    "    # Get Valid time\n",
    "    ds_model_ALL = import_data.get_valid_time(ds_model_ALL)\n",
    "    \n",
    "    # For each variable\n",
    "    for cvar in variables:\n",
    "\n",
    "        # For each init time period\n",
    "        for it in init_slice: \n",
    "            it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "            # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "            # So we need to add one day, so we don't double count.\n",
    "            print(it_start,\"to\",it)\n",
    "\n",
    "            # For each forecast time we haven't plotted yet\n",
    "            #ft_to_plot = ds_status.sel(init_time=it)\n",
    "            #ft_to_plot = ft_to_plot.where(ft_to_plot.isnull(), drop=True).fore_time\n",
    "\n",
    "            for ft in da_slices.values: \n",
    "\n",
    "                #print(ft.astype('timedelta64[D]'))\n",
    "                #cs_str = format(days_2_int_dict[ft], '02') # Get index of current forcast week\n",
    "                #week_str = format(int(ft.astype('timedelta64[D]').astype('int')/Ndays) , '02') # Get string of current week\n",
    "                cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "                cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "                #it_yr = str(pd.to_datetime(it).year) \n",
    "                #it_m = str(pd.to_datetime(it).month)\n",
    "\n",
    "                # Get datetime64 of valid time start and end\n",
    "                valid_start = it_start + ft\n",
    "                valid_end = it + ft\n",
    "\n",
    "                # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "                for metric in metrics_all[cvar]:\n",
    "\n",
    "                    # File paths and stuff\n",
    "                    out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                    if not os.path.exists(out_metric_dir):\n",
    "                        os.makedirs(out_metric_dir) \n",
    "                        \n",
    "                    out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                    if not os.path.exists(out_init_dir):\n",
    "                        os.makedirs(out_init_dir)\n",
    "                        \n",
    "                    out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                    if not os.path.exists(out_mod_dir):\n",
    "                        os.makedirs(out_mod_dir)     \n",
    "                        \n",
    "                    out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "                    # Only update if either we are updating All or it doesn't yet exist\n",
    "                    # OR, its one of the last 3 init times \n",
    "                    if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-3:]):\n",
    "                        #print(\"    Updating...\")\n",
    "\n",
    "                        # Select init period and fore_time of interest\n",
    "                        ds_model = ds_model_ALL.sel(init_time=slice(it_start, it))\n",
    "\n",
    "                        # Check we found any init_times in range\n",
    "                        if ds_model.init_time.size==0:\n",
    "                            #print('init_time not found.')\n",
    "                            continue\n",
    "\n",
    "                        # Select var of interest (if available)\n",
    "                        if cvar in ds_model.variables:\n",
    "                            ds_model = ds_model[cvar]\n",
    "                        else:\n",
    "                            #print('cvar not found.')\n",
    "                            continue\n",
    "\n",
    "                        # Check if we have any valid times in range of target dates\n",
    "                        ds_model = ds_model.where((ds_model.valid_time>=valid_start) & (ds_model.valid_time<=valid_end), drop=True) \n",
    "                        if ds_model.fore_time.size == 0:\n",
    "                            #print(\"no fore_time found for target period.\")\n",
    "                            continue\n",
    "\n",
    "                        # Average over for_time and init_times\n",
    "                        ds_model = ds_model.mean(dim=['fore_time','init_time'])\n",
    "\n",
    "                        if metric=='mean': # Calc ensemble mean\n",
    "                            ds_model = ds_model.mean(dim='ensemble')\n",
    "\n",
    "                        elif metric=='SIP': # Calc probability\n",
    "                            # Remove ensemble members having missing data\n",
    "                            ok_ens = ((ds_model.notnull().sum(dim='x').sum(dim='y'))>0) # select ensemble members with any data\n",
    "                            ds_model = ((ds_model.where(ok_ens, drop=True)>=0.15) ).mean(dim='ensemble').where(ds_model.isel(ensemble=0).notnull())\n",
    "\n",
    "                        elif metric=='anomaly': # Calc anomaly in reference to mean observed 1980-2010\n",
    "                            # Get climatological mean\n",
    "                            da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "                            # Calc anomaly\n",
    "                            ds_model = ds_model.mean(dim='ensemble') - da_obs_mean\n",
    "                            # Add back lat/long (get dropped because of round off differences)\n",
    "                            ds_model['lat'] = da_obs_mean.lat\n",
    "                            ds_model['lon'] = da_obs_mean.lon\n",
    "                        else:\n",
    "                            raise ValueError('metric not implemented')\n",
    "\n",
    "                        # drop ensemble if still present\n",
    "                        if 'ensemble' in ds_model:\n",
    "                            ds_model = ds_model.drop('ensemble')\n",
    "\n",
    "                        ds_model.coords['model'] = cmod\n",
    "                        if 'xm' in ds_model:\n",
    "                            ds_model = ds_model.drop(['xm','ym']) #Dump coords we don't use\n",
    "\n",
    "                        # Add Coords info\n",
    "                        ds_model.name = metric\n",
    "                        ds_model.coords['model'] = cmod\n",
    "                        ds_model.coords['init_start'] = it_start\n",
    "                        ds_model.coords['init_end'] = it\n",
    "                        ds_model.coords['valid_start'] = it_start+ft\n",
    "                        ds_model.coords['valid_end'] = it+ft\n",
    "                        ds_model.coords['fore_time'] = ft\n",
    "                        \n",
    "                        # Save to file\n",
    "                        ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "                        # Clean up for current model\n",
    "                        ds_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#          climatology  trend                             #\n",
    "###########################################################\n",
    "\n",
    "cmod = 'climatology'\n",
    "\n",
    "all_files = os.path.join(mod_dir,cmod,runType,'sipn_nc', str(cd.year)+'*.nc')\n",
    "files = glob.glob(all_files)\n",
    "\n",
    "obs_clim_model = xr.open_mfdataset(sorted(files), \n",
    "        chunks={'time': 30, 'x': 304, 'y': 448},  \n",
    "         concat_dim='time', autoclose=True, parallel=True)\n",
    "\n",
    "# For each variable\n",
    "for cvar in variables:\n",
    "\n",
    "    # For each init time period\n",
    "    for it in init_slice: \n",
    "        it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "        # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "        # So we need to add one day, so we don't double count.\n",
    "        print(it_start,\"to\",it)\n",
    "\n",
    "        for ft in da_slices.values: \n",
    "\n",
    "            cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "            cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "            # Get datetime64 of valid time start and end\n",
    "            valid_start = it_start + ft\n",
    "            valid_end = it + ft\n",
    "\n",
    "            # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "            for metric in metrics_all[cvar]:\n",
    "\n",
    "                # File paths and stuff\n",
    "                out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                if not os.path.exists(out_metric_dir):\n",
    "                    os.makedirs(out_metric_dir) \n",
    "\n",
    "                out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                if not os.path.exists(out_init_dir):\n",
    "                    os.makedirs(out_init_dir)\n",
    "\n",
    "                out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                if not os.path.exists(out_mod_dir):\n",
    "                    os.makedirs(out_mod_dir)     \n",
    "\n",
    "                out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "                # Only update if either we are updating All or it doesn't yet exist\n",
    "                # OR, its one of the last 3 init times \n",
    "                if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-3:]):\n",
    "                    #print(\"    Updating...\")\n",
    "\n",
    "                    # Check if we have any valid times in range of target dates\n",
    "                    ds_model = obs_clim_model[cvar].where((obs_clim_model.time>=valid_start) & (obs_clim_model.time<=valid_end), drop=True) \n",
    "                    if 'time' in ds_model.lat.dims:\n",
    "                        ds_model.coords['lat'] = ds_model.lat.isel(time=0).drop('time') # Drop time from lat/lon dims (not sure why?)\n",
    "\n",
    "                    # If we have any time\n",
    "                    if ds_model.time.size > 0:\n",
    "\n",
    "                        # Average over time\n",
    "                        ds_model = ds_model.mean(dim='time')\n",
    "\n",
    "                        if metric=='mean': # Calc ensemble mean\n",
    "                            ds_model = ds_model\n",
    "                        elif metric=='SIP': # Calc probability\n",
    "                            # Issue of some ensemble members having missing data\n",
    "                            ocnmask = ds_model.notnull()\n",
    "                            ds_model = (ds_model>=0.15).where(ocnmask)\n",
    "                        elif metric=='anomaly': # Calc anomaly in reference to mean observed 1980-2010\n",
    "                            # Get climatological mean\n",
    "                            da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "                            # Get anomaly\n",
    "                            ds_model = ds_model - da_obs_mean\n",
    "                            # Add back lat/long (get dropped because of round off differences)\n",
    "                            ds_model['lat'] = da_obs_mean.lat\n",
    "                            ds_model['lon'] = da_obs_mean.lon\n",
    "                        else:\n",
    "                            raise ValueError('metric not implemented')   \n",
    "\n",
    "                        # Drop un-needed coords to match model format\n",
    "                        if 'doy' in ds_model.coords:\n",
    "                            ds_model = ds_model.drop(['doy'])\n",
    "                        if 'xm' in ds_model.coords:\n",
    "                            ds_model = ds_model.drop(['xm'])\n",
    "                        if 'ym' in ds_model.coords:\n",
    "                            ds_model = ds_model.drop(['ym'])\n",
    "                    \n",
    "                        # Add Coords info\n",
    "                        ds_model.name = metric\n",
    "                        ds_model.coords['model'] = cmod\n",
    "                        ds_model.coords['init_start'] = it_start\n",
    "                        ds_model.coords['init_end'] = it\n",
    "                        ds_model.coords['valid_start'] = it_start+ft\n",
    "                        ds_model.coords['valid_end'] = it+ft\n",
    "                        ds_model.coords['fore_time'] = ft\n",
    "                        \n",
    "                        # Save to file\n",
    "                        ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "                        # Clean up for current model\n",
    "                        ds_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#                               OBSERVATIONS                               #\n",
    "############################################################################\n",
    "\n",
    "cmod = 'Observed'\n",
    "\n",
    "updateAll = True # We ALWAYS want to update all observations, because each day we get new obs that can be used to evaluate forecasts from up to 12 months ago\n",
    "\n",
    "# For each variable\n",
    "for cvar in variables:\n",
    "\n",
    "    # For each init time period\n",
    "    for it in init_slice: \n",
    "        it_start = it-np.timedelta64(Ndays,'D') + np.timedelta64(1,'D') # Start period for init period (it is end of period). Add 1 day because when\n",
    "        # we select using slice(start,stop) it is inclusive of end points. So here we are defining the start of the init AND the start of the valid time.\n",
    "        # So we need to add one day, so we don't double count.\n",
    "        print(it_start,\"to\",it)\n",
    "\n",
    "        for ft in da_slices.values: \n",
    "\n",
    "            cdoy_end = pd.to_datetime(it + ft).timetuple().tm_yday # Get current day of year end for valid time\n",
    "            cdoy_start = pd.to_datetime(it_start + ft).timetuple().tm_yday  # Get current day of year end for valid time\n",
    "\n",
    "            # Get datetime64 of valid time start and end\n",
    "            valid_start = it_start + ft\n",
    "            valid_end = it + ft\n",
    "\n",
    "            # Loop through variable of interest + any metrics (i.e. SIP) based on that\n",
    "            for metric in metrics_all[cvar]:\n",
    "\n",
    "                # File paths and stuff\n",
    "                out_metric_dir = os.path.join(E.model['MME_NEW'][runType]['sipn_nc'], cvar, metric)\n",
    "                if not os.path.exists(out_metric_dir):\n",
    "                    os.makedirs(out_metric_dir) \n",
    "\n",
    "                out_init_dir = os.path.join(out_metric_dir, pd.to_datetime(it).strftime('%Y-%m-%d'))\n",
    "                if not os.path.exists(out_init_dir):\n",
    "                    os.makedirs(out_init_dir)\n",
    "\n",
    "                out_mod_dir = os.path.join(out_init_dir, cmod)\n",
    "                if not os.path.exists(out_mod_dir):\n",
    "                    os.makedirs(out_mod_dir)     \n",
    "\n",
    "                out_nc_file = os.path.join(out_mod_dir, pd.to_datetime(it+ft).strftime('%Y-%m-%d')+'_'+cmod+'.nc')\n",
    "\n",
    "                # Only update if either we are updating All or it doesn't yet exist\n",
    "                # OR, its one of the last 3 init times \n",
    "                if updateAll | (os.path.isfile(out_nc_file)==False) | np.any(it in init_slice[-3:]):\n",
    "                    #print(\"    Updating...\")\n",
    "\n",
    "                    # Check if we have any valid times in range of target dates\n",
    "                    ds_model = da_obs_c = ds_81[cvar].sel(time=slice(valid_start, valid_end))\n",
    "                    \n",
    "                    if 'time' in ds_model.lat.dims:\n",
    "                        ds_model.coords['lat'] = ds_model.lat.isel(time=0).drop('time') # Drop time from lat/lon dims (not sure why?)\n",
    "\n",
    "                    # If we have any time\n",
    "                    if ds_model.time.size > 0:\n",
    "\n",
    "                        if metric=='mean':\n",
    "                            ds_model = ds_model.mean(dim='time') #ds_81.sic.sel(time=(it + ft))\n",
    "                        elif metric=='SIP':\n",
    "                            ds_model = (ds_model >= 0.15).mean(dim='time').astype('int').where(ds_model.isel(time=0).notnull())\n",
    "                        elif metric=='anomaly':\n",
    "                            da_obs_VT = ds_model.mean(dim='time')\n",
    "                            da_obs_mean = mean_1980_2010_sic.isel(time=slice(cdoy_start,cdoy_end)).mean(dim='time')\n",
    "                            ds_model = da_obs_VT - da_obs_mean\n",
    "                        else:\n",
    "                            raise ValueError('Not implemented')\n",
    "\n",
    "                        # Drop coords we don't need\n",
    "                        ds_model = ds_model.drop(['hole_mask','xm','ym'])\n",
    "                        if 'time' in ds_model:\n",
    "                            ds_model = ds_model.drop('time')\n",
    "\n",
    "                        # Add Coords info\n",
    "                        ds_model.name = metric\n",
    "                        ds_model.coords['model'] = cmod\n",
    "                        ds_model.coords['init_start'] = it_start\n",
    "                        ds_model.coords['init_end'] = it\n",
    "                        ds_model.coords['valid_start'] = it_start+ft\n",
    "                        ds_model.coords['valid_end'] = it+ft\n",
    "                        ds_model.coords['fore_time'] = ft\n",
    "\n",
    "                        # Write to disk\n",
    "                        ds_model.to_netcdf(out_nc_file)\n",
    "\n",
    "                        # Clean up for current model\n",
    "                        ds_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:04\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in weekly metrics...\n",
      "    Loading anomaly ...\n",
      "    Found 39 initialization periods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 12\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 14\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 13\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 16\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 15\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  **atop_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading mean ...\n",
      "    Found 39 initialization periods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 12\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 14\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 13\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 16\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 15\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  **atop_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading SIP ...\n",
      "    Found 39 initialization periods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 12\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 14\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 13\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 16\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 15\n",
      "  **atop_kwargs)\n",
      "/home/disk/sipn/nicway/anaconda3/envs/esio/lib/python3.6/site-packages/dask/array/core.py:3468: PerformanceWarning: Increasing number of chunks by factor of 10\n",
      "  **atop_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to Zarr...\n",
      "Finished updating Weekly SIC metrics and saved to Zar\n"
     ]
    }
   ],
   "source": [
    "# Load in all data and write to Zarr\n",
    "# Load in all metrics for given variable\n",
    "print(\"Loading in weekly metrics...\")\n",
    "ds_m = import_data.load_MME_by_init_end(E=E, runType=runType, variable=cvar, \n",
    "                            metrics=metrics_all[cvar], \n",
    "                            init_range=[init_slice[0],init_slice[-1]])\n",
    "\n",
    "# Drop models that we don't evaluate (i.e. monthly means)\n",
    "models_keep = [x for x in ds_m.model.values if x not in ['noaasipn','modcansipns_3','modcansipns_4']]\n",
    "ds_m = ds_m.sel(model=models_keep)\n",
    "# Get list of dynamical models that are not observations\n",
    "dynamical_Models = [x for x in ds_m.model.values if x not in ['Observed','climatology','dampedAnomaly','dampedAnomalyTrend']]\n",
    "# Get list of all models\n",
    "all_Models = [x for x in ds_m.model.values if x not in ['Observed']]\n",
    "# Add MME\n",
    "MME_avg = ds_m.sel(model=dynamical_Models).mean(dim='model') # only take mean over dynamical models\n",
    "MME_avg.coords['model'] = 'MME'\n",
    "ds_ALL = xr.concat([ds_m, MME_avg], dim='model')\n",
    "\n",
    "# Save to Zarr\n",
    "print(\"Saving to Zarr...\")\n",
    "ds_ALL.to_zarr('/home/disk/sipn/nicway/data/model/zarr/sic.zarr', mode='w')\n",
    "print(\"Finished updating Weekly SIC metrics and saved to Zar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.4 esio",
   "language": "python",
   "name": "esio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
