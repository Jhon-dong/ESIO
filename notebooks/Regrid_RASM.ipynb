{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This code is part of the SIPN2 project focused on improving sub-seasonal to seasonal predictions of Arctic Sea Ice. \n",
    "If you use this code for a publication or presentation, please cite the reference in the README.md on the\n",
    "main page (https://github.com/NicWayand/ESIO). \n",
    "\n",
    "Questions or comments should be addressed to nicway@uw.edu\n",
    "\n",
    "Copyright (c) 2018 Nic Wayand\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Standard Imports\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import matplotlib\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ESIO Imports\n",
    "import esio\n",
    "import esiodata as ed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General plotting settings\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context(\"talk\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = ed.esiodata.load()\n",
    "# Directories\n",
    "all_models=['rasmesrl']\n",
    "runType='forecast'\n",
    "updateall = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stero_grid_file = E.obs['NSIDC_0051']['grid']\n",
    "obs_grid = esio.load_grid_info(stero_grid_file, model='NSIDC')\n",
    "# Ensure latitude is within bounds (-90 to 90)\n",
    "# Have to do this because grid file has 90.000001\n",
    "obs_grid['lat_b'] = obs_grid.lat_b.where(obs_grid.lat_b < 90, other = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regridding Options\n",
    "method='nearest_s2d' # ['bilinear', 'conservative', 'nearest_s2d', 'nearest_d2s', 'patch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# - Get mask\n",
    "# - Get lat lon bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dic = {'aice':'sic','lat':'nj','lon':'ni','TLAT':'lat','TLON':'lon'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regridding  rasmesrl ...\n",
      "Found  247  initialization times.\n",
      "Only updating new files\n",
      "Skipping  2017-08-21  already imported.\n",
      "Skipping  2017-10-04  already imported.\n",
      "Skipping  2017-12-14  already imported.\n",
      "Skipping  2017-09-14  already imported.\n",
      "Skipping  2017-12-26  already imported.\n",
      "Skipping  2018-04-02  already imported.\n",
      "Skipping  2017-11-24  already imported.\n",
      "Skipping  2016-11-12  already imported.\n",
      "Skipping  2017-12-04  already imported.\n",
      "Skipping  2017-11-15  already imported.\n",
      "Skipping  2018-02-28  already imported.\n",
      "Skipping  2017-10-08  already imported.\n",
      "Skipping  2017-12-13  already imported.\n",
      "Skipping  2018-04-21  already imported.\n",
      "Skipping  2017-09-24  already imported.\n",
      "Skipping  2018-03-16  already imported.\n",
      "Skipping  2018-03-10  already imported.\n",
      "Skipping  2017-08-11  already imported.\n",
      "Skipping  2017-12-01  already imported.\n",
      "Skipping  2017-11-03  already imported.\n",
      "Skipping  2018-03-01  already imported.\n",
      "Skipping  2018-04-12  already imported.\n",
      "Skipping  2017-07-29  already imported.\n",
      "Skipping  2018-04-18  already imported.\n",
      "Skipping  2018-04-24  already imported.\n",
      "Skipping  2018-03-30  already imported.\n",
      "Skipping  2016-11-07  already imported.\n",
      "Skipping  2016-11-13  already imported.\n",
      "Skipping  2017-08-29  already imported.\n",
      "Skipping  2017-09-20  already imported.\n",
      "Skipping  2016-11-10  already imported.\n",
      "Skipping  2017-09-25  already imported.\n",
      "Skipping  2017-12-16  already imported.\n",
      "Skipping  2018-03-28  already imported.\n",
      "Skipping  2017-09-18  already imported.\n",
      "Skipping  2017-09-17  already imported.\n",
      "Skipping  2018-03-13  already imported.\n",
      "Skipping  2017-09-29  already imported.\n",
      "Skipping  2018-03-06  already imported.\n",
      "Skipping  2017-11-11  already imported.\n",
      "Skipping  2018-04-07  already imported.\n",
      "Skipping  2018-05-10  already imported.\n",
      "Skipping  2016-11-03  already imported.\n",
      "Skipping  2017-08-22  already imported.\n",
      "Skipping  2017-07-31  already imported.\n",
      "Skipping  2016-11-06  already imported.\n",
      "Skipping  2017-10-20  already imported.\n",
      "Skipping  2017-11-26  already imported.\n",
      "Skipping  2018-04-25  already imported.\n",
      "Skipping  2017-11-21  already imported.\n",
      "Skipping  2018-05-04  already imported.\n",
      "Skipping  2017-09-16  already imported.\n",
      "Skipping  2018-03-04  already imported.\n",
      "Skipping  2017-12-05  already imported.\n",
      "Skipping  2017-11-18  already imported.\n",
      "Skipping  2017-08-07  already imported.\n",
      "Skipping  2017-09-10  already imported.\n",
      "Skipping  2017-10-05  already imported.\n",
      "Skipping  2017-08-23  already imported.\n",
      "Skipping  2017-09-13  already imported.\n",
      "Skipping  2016-10-29  already imported.\n",
      "Skipping  2018-02-20  already imported.\n",
      "Skipping  2016-10-28  already imported.\n",
      "Skipping  2018-04-19  already imported.\n",
      "Skipping  2017-11-30  already imported.\n",
      "Skipping  2017-11-20  already imported.\n",
      "Skipping  2018-04-09  already imported.\n",
      "Skipping  2017-09-28  already imported.\n",
      "Skipping  2017-10-07  already imported.\n",
      "Skipping  2017-12-19  already imported.\n",
      "Skipping  2018-04-29  already imported.\n",
      "Skipping  2018-04-14  already imported.\n",
      "Skipping  2017-10-03  already imported.\n",
      "Skipping  2018-03-19  already imported.\n",
      "Skipping  2018-04-03  already imported.\n",
      "Skipping  2017-08-08  already imported.\n",
      "Skipping  2018-04-23  already imported.\n",
      "Skipping  2018-02-14  already imported.\n",
      "Skipping  2017-08-06  already imported.\n",
      "Skipping  2017-09-23  already imported.\n",
      "Skipping  2018-04-11  already imported.\n",
      "Skipping  2017-09-21  already imported.\n",
      "Skipping  2017-10-25  already imported.\n",
      "Skipping  2017-09-15  already imported.\n",
      "Skipping  2018-05-07  already imported.\n",
      "Skipping  2016-10-31  already imported.\n",
      "Skipping  2017-08-20  already imported.\n",
      "Skipping  2018-02-22  already imported.\n",
      "Skipping  2017-11-22  already imported.\n",
      "Skipping  2018-04-28  already imported.\n",
      "Skipping  2017-10-21  already imported.\n",
      "Skipping  2017-11-06  already imported.\n",
      "Skipping  2017-08-12  already imported.\n",
      "Skipping  2018-03-22  already imported.\n",
      "Skipping  2018-04-10  already imported.\n",
      "Skipping  2018-04-27  already imported.\n",
      "Skipping  2017-12-20  already imported.\n",
      "Skipping  2016-11-02  already imported.\n",
      "Skipping  2017-10-02  already imported.\n",
      "Skipping  2017-08-30  already imported.\n",
      "Skipping  2018-02-26  already imported.\n",
      "Skipping  2017-11-01  already imported.\n",
      "Skipping  2018-03-25  already imported.\n",
      "Skipping  2018-05-06  already imported.\n",
      "Skipping  2017-12-03  already imported.\n",
      "Skipping  2017-07-27  already imported.\n",
      "Skipping  2018-03-17  already imported.\n",
      "Skipping  2017-12-24  already imported.\n",
      "Skipping  2017-08-25  already imported.\n",
      "Skipping  2017-09-09  already imported.\n",
      "Skipping  2018-03-18  already imported.\n",
      "Skipping  2017-10-09  already imported.\n",
      "Skipping  2018-02-18  already imported.\n",
      "Skipping  2017-11-27  already imported.\n",
      "Skipping  2018-02-25  already imported.\n",
      "Skipping  2017-09-02  already imported.\n",
      "Skipping  2018-02-21  already imported.\n",
      "Skipping  2017-10-13  already imported.\n",
      "Skipping  2017-10-14  already imported.\n",
      "Skipping  2017-11-13  already imported.\n",
      "Skipping  2018-03-29  already imported.\n",
      "Skipping  2017-09-26  already imported.\n",
      "Skipping  2018-04-17  already imported.\n",
      "Skipping  2017-10-26  already imported.\n",
      "Skipping  2016-10-30  already imported.\n",
      "Skipping  2017-08-15  already imported.\n",
      "Skipping  2017-12-22  already imported.\n",
      "Skipping  2018-05-02  already imported.\n",
      "Skipping  2017-12-10  already imported.\n",
      "Skipping  2018-04-01  already imported.\n",
      "Skipping  2016-11-18  already imported.\n",
      "Skipping  2017-10-28  already imported.\n",
      "Skipping  2017-08-05  already imported.\n",
      "Skipping  2017-11-17  already imported.\n",
      "Skipping  2018-02-27  already imported.\n",
      "Skipping  2017-10-22  already imported.\n",
      "Skipping  2017-12-02  already imported.\n",
      "Skipping  2018-03-14  already imported.\n",
      "Skipping  2017-08-19  already imported.\n",
      "Skipping  2017-10-30  already imported.\n",
      "Skipping  2018-03-03  already imported.\n",
      "Skipping  2017-08-16  already imported.\n",
      "Skipping  2017-08-17  already imported.\n",
      "Skipping  2018-02-19  already imported.\n",
      "Skipping  2017-08-28  already imported.\n",
      "Skipping  2017-09-06  already imported.\n",
      "Skipping  2017-11-14  already imported.\n",
      "Skipping  2017-10-06  already imported.\n",
      "Skipping  2018-05-01  already imported.\n",
      "Skipping  2017-10-16  already imported.\n",
      "Skipping  2017-08-09  already imported.\n",
      "Skipping  2018-05-05  already imported.\n",
      "Skipping  2018-03-20  already imported.\n",
      "Skipping  2017-10-11  already imported.\n",
      "Skipping  2017-12-06  already imported.\n",
      "Skipping  2017-11-07  already imported.\n",
      "Skipping  2017-12-18  already imported.\n",
      "Skipping  2018-03-05  already imported.\n",
      "Skipping  2018-03-26  already imported.\n",
      "Skipping  2018-03-08  already imported.\n",
      "Skipping  2017-10-18  already imported.\n",
      "Skipping  2017-11-02  already imported.\n",
      "Skipping  2017-10-27  already imported.\n",
      "Skipping  2017-08-18  already imported.\n",
      "Skipping  2017-10-10  already imported.\n",
      "Skipping  2017-12-07  already imported.\n",
      "Skipping  2017-11-10  already imported.\n",
      "Skipping  2017-11-19  already imported.\n",
      "Skipping  2017-12-12  already imported.\n",
      "Skipping  2017-09-19  already imported.\n",
      "Skipping  2017-10-23  already imported.\n",
      "Skipping  2017-10-31  already imported.\n",
      "Skipping  2018-03-12  already imported.\n",
      "Skipping  2017-08-26  already imported.\n",
      "Skipping  2017-12-17  already imported.\n",
      "Skipping  2018-04-15  already imported.\n",
      "Skipping  2017-11-16  already imported.\n",
      "Skipping  2017-08-04  already imported.\n",
      "Skipping  2016-11-05  already imported.\n",
      "Skipping  2017-10-15  already imported.\n",
      "Skipping  2017-11-29  already imported.\n",
      "Skipping  2018-02-16  already imported.\n",
      "Skipping  2018-02-24  already imported.\n",
      "Skipping  2017-08-27  already imported.\n",
      "Skipping  2018-03-31  already imported.\n",
      "Skipping  2017-08-31  already imported.\n",
      "Skipping  2017-09-04  already imported.\n",
      "Skipping  2018-05-03  already imported.\n",
      "Skipping  2017-08-13  already imported.\n",
      "Skipping  2017-12-21  already imported.\n",
      "Skipping  2016-11-04  already imported.\n",
      "Skipping  2016-11-11  already imported.\n",
      "Skipping  2017-10-12  already imported.\n",
      "Skipping  2017-11-23  already imported.\n",
      "Skipping  2017-12-09  already imported.\n",
      "Skipping  2018-02-23  already imported.\n",
      "Skipping  2018-03-11  already imported.\n",
      "Skipping  2018-03-24  already imported.\n",
      "Skipping  2018-03-09  already imported.\n",
      "Skipping  2018-02-15  already imported.\n",
      "Skipping  2018-03-21  already imported.\n",
      "Skipping  2017-11-09  already imported.\n",
      "Skipping  2017-11-28  already imported.\n",
      "Skipping  2017-12-25  already imported.\n",
      "Skipping  2017-08-10  already imported.\n",
      "Skipping  2016-11-15  already imported.\n",
      "Skipping  2017-09-03  already imported.\n",
      "Skipping  2017-08-24  already imported.\n",
      "Skipping  2018-03-27  already imported.\n",
      "Skipping  2017-12-08  already imported.\n",
      "Skipping  2018-04-06  already imported.\n",
      "Skipping  2016-11-01  already imported.\n",
      "Skipping  2018-05-08  already imported.\n",
      "Skipping  2017-11-12  already imported.\n",
      "Skipping  2017-11-04  already imported.\n",
      "Skipping  2016-11-14  already imported.\n",
      "Skipping  2016-11-17  already imported.\n",
      "Skipping  2018-03-23  already imported.\n",
      "Skipping  2018-04-26  already imported.\n",
      "Skipping  2016-11-19  already imported.\n",
      "Skipping  2017-09-01  already imported.\n",
      "Skipping  2017-09-30  already imported.\n",
      "Skipping  2016-11-08  already imported.\n",
      "Skipping  2017-10-19  already imported.\n",
      "Skipping  2018-04-05  already imported.\n",
      "Skipping  2018-04-20  already imported.\n",
      "Skipping  2017-11-05  already imported.\n",
      "Skipping  2018-04-08  already imported.\n",
      "Skipping  2017-10-29  already imported.\n",
      "Skipping  2018-04-04  already imported.\n",
      "Skipping  2018-04-13  already imported.\n",
      "Skipping  2018-04-22  already imported.\n",
      "Skipping  2017-11-25  already imported.\n",
      "Skipping  2017-10-17  already imported.\n",
      "Skipping  2017-10-24  already imported.\n",
      "Skipping  2016-11-16  already imported.\n",
      "Skipping  2018-04-30  already imported.\n",
      "Skipping  2017-10-01  already imported.\n",
      "Skipping  2018-05-09  already imported.\n",
      "Skipping  2018-04-16  already imported.\n",
      "Skipping  2017-09-22  already imported.\n",
      "Skipping  2017-09-27  already imported.\n",
      "Skipping  2017-09-05  already imported.\n",
      "Skipping  2018-02-17  already imported.\n",
      "Skipping  2017-12-11  already imported.\n",
      "Skipping  2017-08-14  already imported.\n",
      "Skipping  2018-03-07  already imported.\n"
     ]
    }
   ],
   "source": [
    "for model in all_models:\n",
    "    print('Regridding ', model, '...')\n",
    "    \n",
    "    data_dir = E.model[model][runType]['native']\n",
    "    data_out = E.model[model][runType]['sipn_nc']\n",
    "    model_grid_file = E.model[model]['grid']\n",
    "    \n",
    "    # Files are stored as per time step (about 45 per init_time)\n",
    "    # First parse files to see what unique init_times we have\n",
    "    # ARCu0.08_121_2018042112_t0300.nc\n",
    "    prefix = 'RASM-ESRL'\n",
    "    all_files = sorted(glob.glob(os.path.join(data_dir, prefix+'*.nc')))\n",
    "    init_times = list(set([s.split('_')[1].split('-00')[0] for s in all_files]))\n",
    "    \n",
    "    print(\"Found \",len(init_times),\" initialization times.\")\n",
    "    if updateall:\n",
    "        print(\"Updating all files...\")\n",
    "    else:\n",
    "        print(\"Only updating new files\")\n",
    "\n",
    "\n",
    "    weights_flag = False # Flag to set up weights have been created\n",
    "\n",
    "    # Load land/sea mask file\n",
    "    if os.path.basename(model_grid_file)!='MISSING':\n",
    "        ds_mask = xr.open_mfdataset(model_grid_file)\n",
    "    else:\n",
    "        ds_mask = None\n",
    "\n",
    "    for cf in init_times:\n",
    "        # Check if already imported and skip (unless updateall flag is True)\n",
    "        f_out = os.path.join(data_out, prefix+'_'+cf+'_Stereo.nc') # netcdf file out \n",
    "        if not updateall:\n",
    "            # TODO: Test if the file is openable (not corrupted)\n",
    "            if os.path.isfile(f_out):\n",
    "                print(\"Skipping \", cf, \" already imported.\")\n",
    "                continue # Skip, file already imported\n",
    "\n",
    "        c_files = sorted(glob.glob(os.path.join(data_dir, prefix+'*_'+cf+'*.nc')))\n",
    "        ds = xr.open_mfdataset(c_files, concat_dim='time', decode_times=False, autoclose=True)\n",
    "\n",
    "        # Rename variables per esipn guidelines\n",
    "        ds.rename(var_dic, inplace=True);\n",
    "        ds = ds.drop('time_bounds')\n",
    "\n",
    "        # Format times\n",
    "        ds.coords['init_time'] = np.datetime64(cf)  #np.datetime64(ds.tau.attrs['time_origin'])\n",
    "        ds.coords['tau'] = ds.tau\n",
    "\n",
    "        ds.swap_dims({'time':'tau'}, inplace=True)\n",
    "        ds.rename({'tau':'fore_time'}, inplace=True)\n",
    "        ds.fore_time.attrs['units'] = 'Forecast offset from initial time'\n",
    "        ds = ds.drop(['time'])\n",
    "        ds.coords['fore_time'] = ds.fore_time.astype('timedelta64[h]') \n",
    "        ds.coords['valid_time'] = ds.fore_time + ds.init_time\n",
    "\n",
    "        # Apply masks (if available)\n",
    "        if ds_mask:\n",
    "            print('found mask')\n",
    "            # land_mask is the fraction of native grid cell that is land\n",
    "            # (1-land_mask) is fraction ocean\n",
    "            # Multiply sic by fraction ocean to get actual native grid cell sic\n",
    "            # Also mask land out where land_mask==1\n",
    "            ds = ds * (1 - ds_mask.land_mask.where(ds_mask.land_mask<1))\n",
    "\n",
    "        # Calculate regridding matrix\n",
    "        regridder = xe.Regridder(ds, obs_grid, method, periodic=False, reuse_weights=weights_flag)\n",
    "        weights_flag = True # Set true for following loops\n",
    "\n",
    "        # Add NaNs to empty rows of matrix (forces any target cell with ANY source cells containing NaN to be NaN)\n",
    "        if method=='conservative':\n",
    "            regridder = esio.add_matrix_NaNs(regridder)\n",
    "\n",
    "        # Regrid variables\n",
    "\n",
    "        var_list = []\n",
    "        for cvar in ds.data_vars:\n",
    "            var_list.append(regridder(ds[cvar]))\n",
    "        ds_out = xr.merge(var_list)\n",
    "\n",
    "        # Expand dims\n",
    "        ds_out = esio.expand_to_sipn_dims(ds_out)\n",
    "\n",
    "        # # Save regridded to netcdf file\n",
    "\n",
    "        ds_out.to_netcdf(f_out)\n",
    "        ds_out = None # Memory clean up\n",
    "        ds = None\n",
    "        print('Saved ', f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if weights_flag:\n",
    "    regridder.clean_weight_file()  # clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sic_all = xr.open_mfdataset(f_out)\n",
    "\n",
    "# # Set up plotting info\n",
    "# cmap_sic = matplotlib.colors.ListedColormap(sns.color_palette(\"Blues\", 10))\n",
    "# cmap_sic.set_bad(color = 'red')\n",
    "\n",
    "# # Plot original projection\n",
    "# plt.figure(figsize=(20,10))\n",
    "# ax1 = plt.axes(projection=ccrs.PlateCarree())\n",
    "# ds_p = ds.sic.isel(fore_time=8)\n",
    "# ds_p.plot.pcolormesh(ax=ax1, x='lon', y='lat', \n",
    "#                                  vmin=0, vmax=1,\n",
    "#                                  cmap=matplotlib.colors.ListedColormap(sns.color_palette(\"Blues\", 10)),\n",
    "#                     transform=ccrs.PlateCarree());\n",
    "# ax1.set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())\n",
    "# gl = ax1.gridlines(crs=ccrs.PlateCarree(), linestyle='-')\n",
    "# gl.xlabels_bottom = True\n",
    "# gl.ylabels_left = True\n",
    "# gl.xformatter = LONGITUDE_FORMATTER\n",
    "# gl.yformatter = LATITUDE_FORMATTER\n",
    "# ax1.coastlines(linewidth=0.75, color='black', resolution='50m');\n",
    "\n",
    "# # Plot SIC on target projection\n",
    "# (f, ax1) = esio.polar_axis()\n",
    "# ds_p.plot.pcolormesh(ax=ax1, x='lon', y='lat', \n",
    "#                                      transform=ccrs.PlateCarree(),\n",
    "#                                      cmap=cmap_sic)\n",
    "# ax1.set_title('Original Grid')\n",
    "\n",
    "# # Plot SIC on target projection\n",
    "# (f, ax1) = esio.polar_axis()\n",
    "# ds_p2 = sic_all.sic.isel(init_time=0).isel(fore_time=8).isel(ensemble=0)\n",
    "# ds_p2.plot.pcolormesh(ax=ax1, x='lon', y='lat', \n",
    "#                                      transform=ccrs.PlateCarree(),\n",
    "#                                      cmap=cmap_sic)\n",
    "# ax1.set_title('Target Grid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.6.4 esio",
   "language": "python",
   "name": "esio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
